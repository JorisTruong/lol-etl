include "application.conf"

setl.config {
  spark {
    spark.default.parallelism = "200"
    spark.sql.shuffle.partitions = "200"
  }
}

champsRepository {
    storage = "CSV"
    path = "src/main/resources/inputs/champs.csv"
    inferSchema = "true"
    delimiter = ","
    header = "true"
}

playersRepository {
    storage = "CSV"
    path = "src/main/resources/inputs/participants.csv"
    inferSchema = "true"
    delimiter = ","
    header = "true"
}

statsRepository {
    storage = "CSV"
    path = "src/main/resources/inputs/stats"
    inferSchema = "true"
    delimiter = ","
    header = "true"
}

champPlayersConnector {
    storage = "CSV"
    path = "src/main/resources/outputs/champ-players"
    inferSchema = "true"
    delimiter = ","
    header = "true"
    saveMode = "Overwrite"
}

champStatsRepository {
    storage = "CSV"
    path = "src/main/resources/outputs/champ-stats"
    inferSchema = "true"
    delimiter = ","
    header = "true"
    saveMode = "Overwrite"
}

killsDeathsConnector {
    storage = "CSV"
    path = "src/main/resources/outputs/kills-deaths"
    inferSchema = "true"
    delimiter = ","
    header = "true"
    saveMode = "Overwrite"
}